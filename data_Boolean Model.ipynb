{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<span style=\"font-size:22px;\">Nama: Muhammad Reyhan Aristya</span>",
      "metadata": {},
      "id": "f80e046d"
    },
    {
      "cell_type": "markdown",
      "source": "<span style=\"font-size:22px;\">NIM: A11.2020.12690</span>",
      "metadata": {},
      "id": "88b595eb"
    },
    {
      "cell_type": "markdown",
      "source": "<span style=\"font-size:22px;\">Kelas: STKI-A11.4703</span>",
      "metadata": {},
      "id": "8cbcbcc3"
    },
    {
      "cell_type": "code",
      "source": "import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tokenize import sent_tokenize , word_tokenize\nimport glob\nimport re\nimport os\nimport numpy as np\nimport sys\nStopwords = set(stopwords.words('english'))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "16e2b051"
    },
    {
      "cell_type": "markdown",
      "source": "Mengimpor modul-modul dari pustaka Natural Language Toolkit (NLTK) dan modul-modul Python lainnya untuk melakukan pemrosesan teks. ",
      "metadata": {},
      "id": "2a011a52"
    },
    {
      "cell_type": "code",
      "source": "def finding_all_unique_words_and_freq(words):\n    words_unique = []\n    word_freq = {}\n    for word in words:\n        if word not in words_unique:\n            words_unique.append(word)\n    for word in words_unique:\n        word_freq[word] = words.count(word)\n    return word_freq\ndef finding_freq_of_word_in_doc(word,words):\n    freq = words.count(word)\n        \ndef remove_special_characters(text):\n    regex = re.compile('[^a-zA-Z0-9\\s]')\n    text_returned = re.sub(regex,'',text)\n    return text_returned",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "b3c70d4a"
    },
    {
      "cell_type": "markdown",
      "source": "Membersihkan dan memproses teks agar dapat dianalisis dengan benar, termasuk menghitung frekuensi kata-kata unik dan menghapus karakter khusus yang tidak diperlukan dalam konteks analisis teks.",
      "metadata": {},
      "id": "eece5091"
    },
    {
      "cell_type": "code",
      "source": "all_words = []\ndict_global = {}\nfile_folder = 'C:\\\\Users\\\\Winry\\\\data\\\\*.txt' #UBAH CODE INI SESUAI TEMPAT FILE TXT DILETAKKAN, JIKA TIDAK KODINGAN INI TIDAK AKAN BISA MEMBACA FILE FILE TXT TERSEBUT\nidx = 1\nfiles_with_index = {}\nfor file in glob.glob(file_folder):\n    print(file)\n    fname = file\n    try:\n        with open(file, \"r\", encoding=\"utf-8\") as file: #membuka file, yang akan secara otomatis menutup file setelah selesai digunakan\n            text = file.read()\n    except UnicodeDecodeError: # mengecualikan file yang tidak dapat dibaca\n        print(f\"Error reading {file}, skipping...\")\n        continue\n    \n    text = remove_special_characters(text)\n    text = re.sub(re.compile('\\d'),'',text)\n    sentences = sent_tokenize(text)\n    words = word_tokenize(text)\n    words = [word for word in words if len(words)>1]\n    words = [word.lower() for word in words]\n    words = [word for word in words if word not in Stopwords]\n    dict_global.update(finding_all_unique_words_and_freq(words))\n    files_with_index[idx] = os.path.basename(fname)\n    idx = idx + 1\n    \nunique_words_all = set(dict_global.keys())\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "d38976dc"
    },
    {
      "cell_type": "markdown",
      "source": "Membaca dan memproses teks dari file-file di direktori tertentu, membersihkannya dari karakter khusus, angka, dan kata-kata berhenti, dan menyimpan kata-kata unik.",
      "metadata": {},
      "id": "6962577c"
    },
    {
      "cell_type": "code",
      "source": "class Node:\n    def __init__(self ,docId, freq = None):\n        self.freq = freq\n        self.doc = docId\n        self.nextval = None\n    \nclass SlinkedList:\n    def __init__(self ,head = None):\n        self.head = head",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "2be3a0ea"
    },
    {
      "cell_type": "markdown",
      "source": "Mendefinisikan dua kelas: Node dan SlinkedList, yang merupakan bagian dari struktur data linked list.",
      "metadata": {},
      "id": "dddb3d71"
    },
    {
      "cell_type": "code",
      "source": "linked_list_data = {}\nfor word in unique_words_all:\n    linked_list_data[word] = SlinkedList()\n    linked_list_data[word].head = Node(1,Node)\nword_freq_in_doc = {}\nidx = 1\nfor file in glob.glob(file_folder):\n    print(file)\n    fname = file\n    try:\n        with open(file, \"r\", encoding=\"utf-8\") as file: #membuka file, yang akan secara otomatis menutup file setelah selesai digunakan\n            text = file.read()\n    except UnicodeDecodeError: # mengecualikan file yang tidak dapat dibaca\n        print(f\"Error reading {file}, skipping...\")\n        continue\n    text = remove_special_characters(text)\n    text = re.sub(re.compile('\\d'),'',text)\n    sentences = sent_tokenize(text)\n    words = word_tokenize(text)\n    words = [word for word in words if len(words)>1]\n    words = [word.lower() for word in words]\n    words = [word for word in words if word not in Stopwords]\n    word_freq_in_doc = finding_all_unique_words_and_freq(words)\n    for word in word_freq_in_doc.keys():\n        linked_list = linked_list_data[word].head\n        while linked_list.nextval is not None:\n            linked_list = linked_list.nextval\n        linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n    idx = idx + 1",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "a7d70d02"
    },
    {
      "cell_type": "markdown",
      "source": "Mengonstruksi indeks terbalik (inverted index) dari teks yang diproses.",
      "metadata": {},
      "id": "3bb5d715"
    },
    {
      "cell_type": "code",
      "source": "query = input('Enter your query:')\nquery = word_tokenize(query)\nconnecting_words = []\ncnt = 1\ndifferent_words = []\nbitwise_op = []\nfor word in query:\n    if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n        different_words.append(word.lower())\n    else:\n        connecting_words.append(word.lower())\nprint(connecting_words)\ntotal_files = len(files_with_index)\nzeroes_and_ones = []\nzeroes_and_ones_of_all_words = []\nfor word in (different_words):\n    if word.lower() in unique_words_all:\n        zeroes_and_ones = [0] * total_files\n        linkedlist = linked_list_data[word].head\n        print(word)\n        while linkedlist.nextval is not None:\n            zeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n            linkedlist = linkedlist.nextval\n        zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n    else:\n        print(word,\" not found\")\n        sys.exit()\nprint(zeroes_and_ones_of_all_words)\nfor word in connecting_words:\n    word_list1 = zeroes_and_ones_of_all_words[0]\n    word_list2 = zeroes_and_ones_of_all_words[1]\n    if word == \"and\":\n        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,word_list2)]\n        zeroes_and_ones_of_all_words.remove(word_list1)\n        zeroes_and_ones_of_all_words.remove(word_list2)\n        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n    elif word == \"or\":\n        bitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]\n        zeroes_and_ones_of_all_words.remove(word_list1)\n        zeroes_and_ones_of_all_words.remove(word_list2)\n        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n    elif word == \"not\":\n        bitwise_op = [not w1 for w1 in word_list2]\n        bitwise_op = [int(b == True) for b in bitwise_op]\n        zeroes_and_ones_of_all_words.remove(word_list2)\n        zeroes_and_ones_of_all_words.remove(word_list1)\n        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\nzeroes_and_ones_of_all_words.insert(0, bitwise_op);\n        \nfiles = []    \n# print(zeroes_and_ones_of_all_words)\nlis = zeroes_and_ones_of_all_words[0]\ncnt = 1\nfor index in lis:\n    if index == 1:\n        files.append(files_with_index[cnt])\n    cnt = cnt+1\n    \nprint(files)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "9b9b0ac7"
    },
    {
      "cell_type": "markdown",
      "source": "Memproses dan mengevaluasi kueri pencarian yang dimasukkan oleh pengguna.",
      "metadata": {},
      "id": "e01e165c"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "66c231ff"
    }
  ]
}